model:
  config_path: "config/coe-tiny-hf-v2"
  trust_remote_code: true
  output_dir: "./output/coe-v2-l3t1"
  config:
    use_expert_communication: false
    inner_iter: 1
    num_experts_per_tok: 7
    num_hidden_layers: 3
    hidden_size: 160
    intermediate_size: 855
    kv_lora_rank: 64
    moe_intermediate_size: 110
    n_routed_experts: 63
    num_attention_heads: 10
    num_key_value_heads: 12
    qk_nope_head_dim: 32
    qk_rope_head_dim: 16



training:
  per_device_train_batch_size: 32
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  num_train_epochs: 3
  learning_rate: 1.0e-3
  max_grad_norm: 100.0
  weight_decay: 0.01
  save_steps: 0
  logging_steps: 1
  fp16: false
  prediction_loss_only: true

optimizer:
  type: "AdamW"
  beta1: 0.9
  beta2: 0.999
  epsilon: 1.0e-8

tokenizer:
  path: "config/coe-tiny-hf-v2"
  # max_length: 2048
  max_length: 1024
  padding: "max_length"
  return_tensors: "pt"

data:
  # name: "hoskinson-center/proof-pile"
  name: "iohadrubin/wikitext-103-raw-v1"
  config: null
  split: "train"
  sample_size: 1000000
  preprocessing:
    # min_char_length: 2000
    min_char_length: 50
    truncation: true
    # max_length: 2048
    max_length: 1024
    padding: "max_length"
    return_tensors: "pt"
    remove_columns: true

scheduler:
  type: "linear"
  warmup_steps: 500

evaluation:
  do_eval: true
  eval_steps: 10
  val_size: 32
  per_device_eval_batch_size: 8
  load_best_model_at_end: true

checkpointing:
  save_total_limit: 3
  save_strategy: "steps"