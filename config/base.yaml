# config/base.yaml
# Comprehensive configuration for transformer model pretraining

model:
  config_path: "config/comoe-tiny-hf"
  trust_remote_code: true
  output_dir: "./comoe_pretrained"

tokenizer:
  path: "config/r1-tokenizer-hf"
  max_length: 2048
  padding: "max_length"
  return_tensors: "pt"

data:
  name: "wikitext"
  config: "wikitext-103-v1"
  split: "train"
  sample_size: 1000000
  preprocessing:
    truncation: true
    max_length: 2048
    padding: "max_length"
    return_tensors: "pt"
    remove_columns: true

training:
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 128
  num_train_epochs: 3
  learning_rate: 1.0e-5
  weight_decay: 0.1
  save_steps: 100
  logging_steps: 1
  fp16: true
  prediction_loss_only: true
  
optimizer:
  type: "AdamW"
  beta1: 0.9
  beta2: 0.95
  epsilon: 1.0e-8

scheduler:
  type: "linear"
  warmup_steps: 500
  
evaluation:
  do_eval: false
  eval_steps: 1000
  
checkpointing:
  save_total_limit: 3
  save_strategy: "steps"